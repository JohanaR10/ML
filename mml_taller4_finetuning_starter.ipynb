{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohanaR10/ML/blob/main/mml_taller4_finetuning_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mastering Machine Learning 2025\n",
        "\n",
        "Taller 4: finetuning de modelos de lenguaje para clasificación de texto\n",
        "\n",
        "Antes de iniciar abra este cuaderno en Google Colab y habilite la ejecución con GPU:\n",
        "- En el menú Entorno de ejecución seleccione Cambiar tipo entorno de ejecución.\n",
        "- Asegúrese de tener seleccionado Python 3.\n",
        "- Como Acelerador de hardware seleccione GPU T4.\n",
        "\n"
      ],
      "metadata": {
        "id": "FxbLhYKxD9pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instale las dependencias para asegurar la correcta ejecución del cuaderno."
      ],
      "metadata": {
        "id": "kOARfXCfFLvS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIf7rZpskmIT"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets evaluate huggingface_hub setfit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos el dataset de reseñas de pelítuclas de Rotten Tomatoes, disponible en Hugging Face"
      ],
      "metadata": {
        "id": "uR1DwLy0FY0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSDm6VmFGvot"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset('cornell-movie-review-data/rotten_tomatoes')\n",
        "train_data, test_data = data[\"train\"], data[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning de modelos pre-entrenados\n",
        "\n",
        "Como modelo pre-entrenado usaremos el modelo bert-base-cased, entrenado usando documentos de Wikipedia en inglés.\n",
        "\n",
        "Note que cargamos el modelo para clasificación de secuencias y definimos el número de etiquetas (labels) igual a 2, lo cual es necesario para crear la red neuronal que se usará sobre el modelo pre-entrenado. También definimos el tokenizador correspondiente al modelo."
      ],
      "metadata": {
        "id": "3tSORg5wORIZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3l7ehNPk9Dp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_id = \"bert-base-cased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=2\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un data collator para agregar datos en lotes usando padding para ajustar todas las secuencias a la longitud de la más larga."
      ],
      "metadata": {
        "id": "CUD-sBdOHX3P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTw94CIzlKWn"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una función para tokenizar los datos y la usamos en los datos de train y test."
      ],
      "metadata": {
        "id": "He13j29lHsSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "   \"\"\"Tokeniza datos de entrada (examples)\"\"\"\n",
        "   return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
        "tokenized_test = test_data.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "eL9IrGirHr3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una función para calcular las métricas de interés, en este caso el f1-score."
      ],
      "metadata": {
        "id": "oDgMNUSLH5iA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRdOcMkklOPB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   \"\"\"Calculate F1 score\"\"\"\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "   load_f1 = evaluate.load(\"f1\")\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "   return {\"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los objetos para realizar el entrenamiento."
      ],
      "metadata": {
        "id": "X3voYVXsIfRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4SvM0LGAagu"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Argumentos para el entrenamiento (finetuning) del modelo\n",
        "training_args = TrainingArguments(\n",
        "   \"model\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=1,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        "   report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer para realizar el entrenamiento (finetuning)\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de hacer el finetuning, evaluemos el modelo."
      ],
      "metadata": {
        "id": "Ac8I2yohLBlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "5DwH7SwuLGkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora entrenemos y evaluemos el modelo."
      ],
      "metadata": {
        "id": "sbuzmOFIImsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "CER5DE6GKNA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atxU55GZCESM"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Congelar capas\n",
        "\n",
        "Ahora buscamos hacer un finetuning del modelo BERT, pero congelando algunas de sus capas."
      ],
      "metadata": {
        "id": "KEodjEPOOavN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empezamos definiendo de nuevo el modelo y el tokenizer, como se hizo anteriormente."
      ],
      "metadata": {
        "id": "-uLtYZUVJoTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlw1kgBylPKh"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=2\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para explorar las capas del modelo BERT, imprimimos sus nombres"
      ],
      "metadata": {
        "id": "JSauTegcKW0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_osgStSwlbMo"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora pasamos a congelar todas las capas, excepto la cabeza de clasificación, cuyo nombre empieza con \"classifier\". Esto se hace modificando el parámetro requires_grad para que no entre como parte del cálculo de gradiente en el entrenamiento."
      ],
      "metadata": {
        "id": "8im3oRSGKkj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "     if name.startswith(\"classifier\"):\n",
        "        param.requires_grad = True\n",
        "     else:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "78CiiXYGOi5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instanciamos el trainer para ejecutar el entrenamieno"
      ],
      "metadata": {
        "id": "82YmZSgBLUCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss14iHyUlk_i"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos y evaluamos el modelo"
      ],
      "metadata": {
        "id": "xr_0GTW6LcfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "EimpVA7GLTJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3myc5GN8CvU3"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora buscaremos congelar menos capas para tratar de mejorar el desempeño del modelo."
      ],
      "metadata": {
        "id": "QUuMsLaWMccX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el modelo nuevamente"
      ],
      "metadata": {
        "id": "WRH8W4sIMkR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SZwsQ8BlsHb"
      },
      "outputs": [],
      "source": [
        "model_id = \"bert-base-cased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=2\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora procedemos a congelar todas las capas hasta el bloque encoder 9, dejando las demás habilitadas para entrenamiento. Usamos el índice 165 de los parámetros, que indica el índice del conjunto de parámetros (o layers como aparecen listados anteriormente)."
      ],
      "metadata": {
        "id": "MUKh2j6mMqcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, (name, param) in enumerate(model.named_parameters()):\n",
        "    if index < 165:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "srbZaLqvMp-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instanciamos y entrenamos el modelo"
      ],
      "metadata": {
        "id": "tE58142ENGE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "JDxBuCzWNFMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluamos el modelo"
      ],
      "metadata": {
        "id": "wavDlzl-NJ3t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bp-0t29lvbJ"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few shot classification\n",
        "\n",
        "Ahora realizamos la tarea de clasificación usando pocos ejemplos (few shot classification)."
      ],
      "metadata": {
        "id": "ajmSsFdjPZBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empezamos por seleccionar algunos ejemplos para la tarea, y así simular que tenemos solo 16 casos de cada clase."
      ],
      "metadata": {
        "id": "i1WP2-ImO19d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Kwu6TbJlyIi"
      },
      "outputs": [],
      "source": [
        "from setfit import sample_dataset\n",
        "\n",
        "sampled_train_data = sample_dataset(data[\"train\"], num_samples=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que usamos la librería setfit para esta tarea, la cual requiere que definamos el modelo de embedding que usaremos, en este caso el all-mpnet-base-v2."
      ],
      "metadata": {
        "id": "Cag7mhS9PNft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vxBj6zdl27L"
      },
      "outputs": [],
      "source": [
        "from setfit import SetFitModel\n",
        "\n",
        "model = SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora definimos los argumentos para el entrenamiento del modelo"
      ],
      "metadata": {
        "id": "mG1yhsgWPyoY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEYXKEiel9pO"
      },
      "outputs": [],
      "source": [
        "from setfit import TrainingArguments as SetFitTrainingArguments\n",
        "from setfit import Trainer as SetFitTrainer\n",
        "\n",
        "args = SetFitTrainingArguments(\n",
        "    num_epochs=3,     # Número de epocas para el algoritmo de contrastive learning\n",
        "    num_iterations=20  # Número de parejas de texto a usar\n",
        ")\n",
        "args.eval_strategy = args.evaluation_strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instaciamos el entrenador"
      ],
      "metadata": {
        "id": "-jyJfsGdQWWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SetFitTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=sampled_train_data,\n",
        "    eval_dataset=test_data,\n",
        "    metric=\"f1\"\n",
        ")"
      ],
      "metadata": {
        "id": "7WHt0QYvQW1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos el modelo. Para entrenar el modelo necesitará crear una cuenta weights and biases (https://wandb.ai), buscar su API key e ingresarla en la caja de ejecución."
      ],
      "metadata": {
        "id": "cQg9POYYQdMQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx-0yLkzmAHf"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluamos el modelo"
      ],
      "metadata": {
        "id": "mtqQYO13Qjbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "8AYLKrEnQi9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-entrenamiento y finetuning continuo\n",
        "\n",
        "<font color='red' style=\"font-size:70px\">\n",
        "Nota: NO ejecute esta sección inicialmente, hágalo solo al final del taller\n",
        "</font>\n",
        "\n",
        "\n",
        "\n",
        "Hasta el momento hemos empleado un modelo pre-entrenado al cual le realizamos un finetuning. Ahora consideraremos un pre-enrenamiento adicional del modelo pre-entrenado, el cual usa enmascaramiento  (masked)."
      ],
      "metadata": {
        "id": "XwIn_QgFQBUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empezamos definiendo el modelo y su tokenizador."
      ],
      "metadata": {
        "id": "1lphuVAuYSNj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdzZFP8WmCa1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "model_id = \"bert-base-cased\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos nuestra función de tokenización y la empleamos para tokenizar los datos de entrenamiento y prueba."
      ],
      "metadata": {
        "id": "GndVJJOJYdjS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKvr9-HWmGew"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
        "tokenized_train = tokenized_train.remove_columns(\"label\")\n",
        "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
        "tokenized_test = tokenized_test.remove_columns(\"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora definimos un data collator para modelado de lenguaje que usa enmascaramiendo (masking), con una probabilidad de enmascarar un token (borrarlo de los datos de entrenamiento para usar como etiquetas)."
      ],
      "metadata": {
        "id": "tc57QoXwYs9S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX3KW9R9mKjQ"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Data collator con enmascaramiento\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los argumentos de entrenamiento e instanciamos el entrenador."
      ],
      "metadata": {
        "id": "06zER58XZJF5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JadnT8UDmNxF"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "   \"model\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=10,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        "   report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardamos el tokenizador antes del entrenamiento"
      ],
      "metadata": {
        "id": "e2uIPOVuZnLp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq_mz35imRZ3"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"mlm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos el modelo y lo guardamos"
      ],
      "metadata": {
        "id": "5f_saAhvZuUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"mlm\")"
      ],
      "metadata": {
        "id": "LhPj9IebZsSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evaluar los modelos, usamos una predicción sobre una frase enmascarada, tal que el modelo realice la predicción de la palabra que debe reemplazar \"[MASK]\". Primero usamos el modelo pre-entrenado inicial."
      ],
      "metadata": {
        "id": "jlLxlTeXbUcp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeKbdL2tVzDJ"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
        "preds = mask_filler(\"What a horrible [MASK]!\")\n",
        "\n",
        "for pred in preds:\n",
        "    print(f\"{pred[\"sequence\"]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora cargamos el modelo mlm que preentrenamos con los datos de reseñas de películas y realizamos la misma tarea."
      ],
      "metadata": {
        "id": "NvJ7eE2qbxJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler = pipeline(\"fill-mask\", model=\"mlm\")\n",
        "preds = mask_filler(\"What a horrible [MASK]!\")\n",
        "\n",
        "for pred in preds:\n",
        "    print(f\"{pred[\"sequence\"]}\")"
      ],
      "metadata": {
        "id": "spIfIpkFQ1SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos continuar con la tarea de finetuning, definiendo el modelo como antes."
      ],
      "metadata": {
        "id": "ZCo1oEJWb7M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"mlm\", num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mlm\")"
      ],
      "metadata": {
        "id": "pgSJOF1bQ3Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De aquí en adelante los pasos siguen igual que lo hicimos anteriormente."
      ],
      "metadata": {
        "id": "K7dYwxQvb_ap"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}